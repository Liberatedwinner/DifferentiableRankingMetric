{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_uc=5\n",
    "min_sc=3\n",
    "dset = 'epinion'\n",
    "\n",
    "if dset == 'ml-20m':\n",
    "    raw_data = pd.read_csv(os.path.join('data', 'ml-20m', 'ratings.csv'))\n",
    "    raw_data.columns = ['userId', 'movieId', 'rating', 'ts']\n",
    "    print(raw_data)\n",
    "    raw_data = raw_data[raw_data['rating'] > 3.5]\n",
    "    n_heldout_users = 10000\n",
    "\n",
    "# elif dset == \"ml-1m\":\n",
    "#     raw_data = pd.read_csv(os.path.join('data', 'ml-1m', 'ratings.dat'), header=None, sep='::')\n",
    "#     raw_data.columns = ['userId', 'movieId', 'rating', 'ts']\n",
    "#     print(raw_data)\n",
    "#     raw_data = raw_data[raw_data['rating'] > 3.5]\n",
    "#     n_heldout_users = 1000\n",
    "    \n",
    "elif dset == 'sk':\n",
    "    raw_data = pd.read_csv(os.path.join('data', 'sk', 'model_likes_anon.psv'), \n",
    "                           delimiter='|', \n",
    "                           quotechar='\\\\')\n",
    "    raw_data['userId'] = raw_data['uid'].astype(\"object\")\n",
    "    raw_data['movieId'] = raw_data['mid'].astype(\"object\")\n",
    "    print(raw_data)\n",
    "    n_heldout_users = 1500\n",
    "    min_sc = 4\n",
    "    \n",
    "elif dset == 'melon':\n",
    "    raw_data = pd.read_json(os.path.join('data', 'melon', 'train.json'))\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for i, r in raw_data.iterrows():\n",
    "        rows.extend([i] * len(r.songs))\n",
    "        cols.extend(r.songs)\n",
    "    raw_data = pd.DataFrame({\"userId\": rows, \"movieId\": cols})\n",
    "    min_sc = 10\n",
    "    n_heldout_users = 10000\n",
    "\n",
    "elif dset == 'epinion':\n",
    "    import scipy.io\n",
    "    rat = scipy.io.loadmat(\"data/epinion/rating_with_timestamp.mat\")['rating_with_timestamp']\n",
    "    u = rat[:, 0]\n",
    "    i = rat[:, 1]\n",
    "    r = rat[:, 3]\n",
    "    raw_data = pd.DataFrame({\n",
    "        'userId' : u,\n",
    "        'movieId': i,\n",
    "        'rating': r,\n",
    "    })\n",
    "    n_heldout_users = 2000\n",
    "    \n",
    "# elif dset == 'ciao':\n",
    "#     import scipy.io\n",
    "#     rat = scipy.io.loadmat(\"data/ciao/rating_with_timestamp.mat\")['rating']\n",
    "#     u = rat[:, 0]\n",
    "#     i = rat[:, 1]\n",
    "#     r = rat[:, 3]\n",
    "#     raw_data = pd.DataFrame({\n",
    "#         'userId' : u,\n",
    "#         'movieId': i,\n",
    "#         'rating': r,\n",
    "#     })\n",
    "#     n_heldout_users = 1000\n",
    "    \n",
    "# elif dset == 'pinterest':\n",
    "#     import json\n",
    "#     with open(os.path.join('data', 'pinterest_iccv', '_pins2.json'),'r') as f:\n",
    "#         data = json.load(f)\n",
    "#     with open(os.path.join('data', 'pinterest_iccv', '_pin_im2.json'),'r') as f:\n",
    "#         b = json.load(f)\n",
    "#         pid_to_imname = {x['pin_id']:x['im_name'] for x in b}\n",
    "#     uids = []\n",
    "#     iids = []\n",
    "#     rats = []\n",
    "#     for e, row in enumerate(data): \n",
    "#         x = row['board_id']\n",
    "#         y = row['pins']\n",
    "#         uids.extend([e for _ in range(len(y))])\n",
    "#         iids.extend([pid_to_imname[x] for x in y])\n",
    "#         rats.extend([1 for _ in range(len(y))])\n",
    "#     raw_data = pd.DataFrame({\n",
    "#         'userId' : uids,\n",
    "#         'movieId': iids,\n",
    "#         'rating': rats,\n",
    "#     })\n",
    "#     min_sc = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep items that are clicked on by at least 5 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=5, min_sc=min_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 640918 watching events from 21396 users and 59377 movies (sparsity: 99.950%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. - _raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (_raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = _raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 76156,  11200,  62087,  76767,  90790,  52510,  76919,   1617,\n",
       "             58161,  93044,\n",
       "            ...\n",
       "            121718, 110462,  20836,  37744, 117129,   2082,  38309, 128556,\n",
       "             70000, 136725],\n",
       "           dtype='int64', name='userId', length=136675)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "#n_heldout_users = 3000\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116675, 10000, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_users), len(vd_users), len(te_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        \n",
    "        n_items_u = len(group)\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"%d users sampled\" % (1+i))\n",
    "            sys.stdout.flush()\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 users sampled\n",
      "200 users sampled\n",
      "300 users sampled\n",
      "400 users sampled\n",
      "500 users sampled\n",
      "600 users sampled\n",
      "700 users sampled\n",
      "800 users sampled\n",
      "900 users sampled\n",
      "1000 users sampled\n",
      "1100 users sampled\n",
      "1200 users sampled\n",
      "1300 users sampled\n",
      "1400 users sampled\n",
      "1500 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 users sampled\n",
      "200 users sampled\n",
      "300 users sampled\n",
      "400 users sampled\n",
      "500 users sampled\n",
      "600 users sampled\n",
      "700 users sampled\n",
      "800 users sampled\n",
      "900 users sampled\n",
      "1000 users sampled\n",
      "1100 users sampled\n",
      "1200 users sampled\n",
      "1300 users sampled\n",
      "1400 users sampled\n",
      "1500 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    row, col = tp.userId.apply(lambda x: profile2id[x]).tolist(), tp.movieId.apply(lambda x: show2id[x]).tolist()\n",
    "    return sparse.coo_matrix((np.ones_like(row), (row, col)), shape=(len(profile2id), len(show2id))).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)\n",
    "val_tr = numerize(vad_plays_tr)\n",
    "val_te = numerize(vad_plays_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_tr = numerize(test_plays_tr)\n",
    "te_te = numerize(test_plays_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12562\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(np.arange(n_users)[np.asarray(train_data.sum(1)).ravel() > 0]))\n",
    "print(len(np.arange(n_users)[np.asarray(val_tr.sum(1)).ravel() > 0]))\n",
    "print(len(np.arange(n_users)[np.asarray(te_tr.sum(1)).ravel() > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.arange(n_users)\n",
    "tr_users = r[np.asarray(train_data.sum(1)).ravel() > 0]\n",
    "val_users = r[np.asarray(val_tr.sum(1)).ravel() > 0]\n",
    "te_users = r[np.asarray(te_tr.sum(1)).ravel() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23660119482528905"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_te.nnz / val_tr.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23533681425460676"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_te.nnz / te_tr.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"data/parsed/{dset}-new\", 'wb') as f:\n",
    "    pickle.dump((tr_users, val_users, te_users, train_data, val_tr, val_te, te_tr, te_te), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implicit.evaluation import train_test_split\n",
    "total = (train_data + val_tr + te_tr + val_te + te_te)\n",
    "train, te = train_test_split(total, 0.8)\n",
    "tr, val = train_test_split(train, 0.875)\n",
    "with open(f'data/parsed/{dset}-712-', 'wb') as f:\n",
    "    pickle.dump((tr, val, te), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1                                     mp-cml-pt.py\r\n",
      " adam_clr.pkl                          mp-our-ae.py\r\n",
      " adam_dot.pkl                          mp-ours-pt.py\r\n",
      " adam_warp.pkl                         ncf-pt.py\r\n",
      " a.out                                 \u001b[0m\u001b[01;34mneuralsort\u001b[0m/\r\n",
      " asdasd                                \u001b[01;34mold\u001b[0m/\r\n",
      " autoencoder-pt.py                     opop.py\r\n",
      " backup.sh                             ours-fb.py\r\n",
      " \u001b[01;34mbest_res\u001b[0m/                             ours-pt.py\r\n",
      " cdae-fb.py                            ours-trace.py\r\n",
      " \u001b[01;34mciao\u001b[0m/                                'Remark test.ipynb'\r\n",
      " CMAE.ipynb                            res1541.pkl\r\n",
      " cml-fb.py                             res2.pkl\r\n",
      " cml-pt.py                             res33123323.pkl\r\n",
      " cmltemp                               res33323.pkl\r\n",
      " cml-trace.py                          res.pkl\r\n",
      " DAE-ff.ipynb                          rmsprop_dot.pkl\r\n",
      " DAE-Finetuning.ipynb                  rmsprop_warp.pkl\r\n",
      " DAE.ipynb                             run-cml0.sh\r\n",
      " DAE-mgr.ipynb                         run-cml.sh\r\n",
      " DAE-motivation.ipynb                  run-ours0.sh\r\n",
      " \u001b[01;34mdata\u001b[0m/                                 run-ours.sh\r\n",
      " data-prepro-712.ipynb                 run-wmf.sh\r\n",
      " data-prepro.ipynb                     \u001b[01;34msaved_models\u001b[0m/\r\n",
      " data-prepro-lko.ipynb                 \u001b[01;34mscripts\u001b[0m/\r\n",
      " datastat.ipynb                        sk-f3.pdf\r\n",
      " \u001b[01;34meval\u001b[0m/                                'sk-k and lambda1.pdf'\r\n",
      " exp1-sk.ipynb                        'sk-k and lambda2.pdf'\r\n",
      " exp2.py                              'sk-k and lambda.pdf'\r\n",
      " exp3-Copy1.ipynb                      sk-map3.pdf\r\n",
      "'exp3-k and lambda.ipynb'              sk-ndcg3.pdf\r\n",
      " exp3-ml-20m.ipynb                     sk-ndcg-black.pdf\r\n",
      " exp3-sk.ipynb                         sk-ndcg.pdf\r\n",
      " Final_Plot2.pdf                       sk-ndcg-white.pdf\r\n",
      " \u001b[01;35mFinal_Plot2.png\u001b[0m                      'sk-rho and lambda.pdf'\r\n",
      " Final_Plot3.pdf                       slim-fb.py\r\n",
      " Final_Plot.pdf                        slim-pt.py\r\n",
      " \u001b[01;35mFinal_Plot.png\u001b[0m                        slim-test-Copy1.ipynb\r\n",
      " \u001b[01;34mfism\u001b[0m/                                 slim-test.ipynb\r\n",
      " G                                     \u001b[01;34mspotlight\u001b[0m/\r\n",
      " G.pub                                 \u001b[01;34mstorage\u001b[0m/\r\n",
      " grad_check_interpretationtest.ipynb   test_script.py\r\n",
      " grad_check.ipynb                     'Times New Roman 400.ttf'\r\n",
      " implicit-fb.py                       'Times New Roman Gras 700.ttf'\r\n",
      " implicit-pt.py                       'Times New Roman Gras Italique 700.ttf'\r\n",
      " implicit-trace.py                    'Times New Roman Italique 400.ttf'\r\n",
      "'melon-k and lambda1.pdf'              tmp.cml\r\n",
      "'melon-k and lambda2.pdf'              tmp.txt\r\n",
      "'melon-k and lambda.pdf'               \u001b[01;34mtrace\u001b[0m/\r\n",
      "'melon-rho and lambda.pdf'             Untitled1.ipynb\r\n",
      " MF-Finetuning.ipynb                   Untitled2.ipynb\r\n",
      " MF-Motivation-Copy1.ipynb             Untitled-Copy1.ipynb\r\n",
      " MF-Motivation-Copy2.ipynb             vae-fb.py\r\n",
      " MF-Motivation-Copy3.ipynb             VAE-ff.ipynb\r\n",
      " MF-Motivation.ipynb                   VAE-Finetuning-Copy1.ipynb\r\n",
      " \u001b[01;34mmisc\u001b[0m/                                 VAE-Finetuning.ipynb\r\n",
      " ml-20m-f3.pdf                         VAE.ipynb\r\n",
      " ml-20m-map2.pdf                       VAE-motivation.ipynb\r\n",
      " ml-20m-map3.pdf                       VAE-motivation-map5.ipynb\r\n",
      " ml-20m-map.pdf                        VAE-TW.ipynb\r\n",
      " ml-20m-ndcg3.pdf                      view.ipynb\r\n",
      " ml-20m-ndcg-black.pdf                 warp-fb.py\r\n",
      " ml-20m-ndcg.pdf                       warp-pt.py\r\n",
      " ml-20m-ndcg-white.pdf                 warp-trace.py\r\n",
      " \u001b[01;34mmodels\u001b[0m/                               xCLIMF-PT.py\r\n",
      " motiv-res.ipynb                      'x = torch.randn(100,100)'\r\n",
      " mp-bpr-pt.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<104645x81219 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2873874 stored elements in Compressed Sparse Row format>, <104645x81219 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 411065 stored elements in Compressed Sparse Row format>, <104645x81219 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 821230 stored elements in Compressed Sparse Row format>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "dset = 'melon'\n",
    "with open(f'data/parsed/{dset}-712', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
