{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ita/code/neurank/code\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘drm_data’: File exists\n",
      "mkdir: cannot create directory ‘drm_data/parsed’: File exists\n"
     ]
    }
   ],
   "source": [
    "# please input the dataset name in this list: ['ml-20m', 'sk', 'epinion', 'melon']\n",
    "!mkdir drm_data\n",
    "!mkdir drm_data/parsed\n",
    "dset = 'ml-20m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-13 20:13:54--  http://files.grouplens.org/datasets/movielens/ml-20m.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 198702078 (189M) [application/zip]\n",
      "Saving to: ‘drm_data/ml-20m.zip’\n",
      "\n",
      "drm_data/ml-20m.zip  40%[=======>            ]  76.42M  21.0KB/s    eta 58m 31s"
     ]
    }
   ],
   "source": [
    "min_uc=5\n",
    "min_sc=3\n",
    "\n",
    "if dset == 'ml-20m':\n",
    "    !wget -c -O drm_data/ml-20m.zip http://files.grouplens.org/datasets/movielens/ml-20m.zip\n",
    "    !unzip -d drm_data/ drm_data/ml-20m.zip\n",
    "    raw_data = pd.read_csv(os.path.join('drm_data', 'ml-20m', 'ratings.csv'))\n",
    "    print(raw_data)\n",
    "    raw_data.columns = ['userId', 'movieId', 'rating', 'ts']\n",
    "    raw_data = raw_data[raw_data['rating'] > 3.5]\n",
    "    n_heldout_users = 10000\n",
    "    \n",
    "    \n",
    "elif dset == 'sk':\n",
    "    !mkdir drm_data/sk\n",
    "    !wget -c -O drm_data/sk/model_likes_anon.psv https://raw.githubusercontent.com/EthanRosenthal/rec-a-sketch/master/data/model_likes_anon.psv\n",
    "    raw_data = pd.read_csv(os.path.join('drm_data', 'sk', 'model_likes_anon.psv'), \n",
    "                           delimiter='|', \n",
    "                           quotechar='\\\\')\n",
    "    print(raw_data)\n",
    "    raw_data['userId'] = raw_data['uid'].astype(\"object\")\n",
    "    raw_data['movieId'] = raw_data['mid'].astype(\"object\")\n",
    "    n_heldout_users = 1500\n",
    "    min_sc = 4\n",
    "    \n",
    "elif dset == 'melon':\n",
    "    raw_data = pd.read_json(os.path.join('drm_data', 'melon', 'train.json'))\n",
    "    print(raw_data)\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for i, r in raw_data.iterrows():\n",
    "        rows.extend([i] * len(r.songs))\n",
    "        cols.extend(r.songs)\n",
    "    raw_data = pd.DataFrame({\"userId\": rows, \"movieId\": cols})\n",
    "    min_sc = 10\n",
    "    n_heldout_users = 10000\n",
    "\n",
    "elif dset == 'epinion':\n",
    "    !wget -c -O drm_data/epinion.zip https://www.cse.msu.edu/~tangjili/datasetcode/epinions_with_rating_timestamp.zip\n",
    "    !unzip -d drm_data/ drm_data/epinion.zip\n",
    "    import scipy.io\n",
    "    rat = scipy.io.loadmat(\"drm_data/epinion_with_rating_timestamp/rating_with_timestamp.mat\")['rating_with_timestamp']\n",
    "    u = rat[:, 0]\n",
    "    i = rat[:, 1]\n",
    "    r = rat[:, 3]\n",
    "    raw_data = pd.DataFrame({\n",
    "        'userId' : u,\n",
    "        'movieId': i,\n",
    "        'rating': r,\n",
    "    })\n",
    "    print(raw_data)\n",
    "    n_heldout_users = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep items that are clicked on by at least 5 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=5, min_sc=min_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 559119 watching events from 15562 users and 28713 movies (sparsity: 99.875%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. - _raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (_raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = _raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12562, 1500, 1500)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_users), len(vd_users), len(te_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        \n",
    "        n_items_u = len(group)\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"%d users sampled\" % (1+i))\n",
    "            sys.stdout.flush()\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 users sampled\n",
      "200 users sampled\n",
      "300 users sampled\n",
      "400 users sampled\n",
      "500 users sampled\n",
      "600 users sampled\n",
      "700 users sampled\n",
      "800 users sampled\n",
      "900 users sampled\n",
      "1000 users sampled\n",
      "1100 users sampled\n",
      "1200 users sampled\n",
      "1300 users sampled\n",
      "1400 users sampled\n",
      "1500 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 users sampled\n",
      "200 users sampled\n",
      "300 users sampled\n",
      "400 users sampled\n",
      "500 users sampled\n",
      "600 users sampled\n",
      "700 users sampled\n",
      "800 users sampled\n",
      "900 users sampled\n",
      "1000 users sampled\n",
      "1100 users sampled\n",
      "1200 users sampled\n",
      "1300 users sampled\n",
      "1400 users sampled\n",
      "1500 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    row, col = tp.userId.apply(lambda x: profile2id[x]).tolist(), tp.movieId.apply(lambda x: show2id[x]).tolist()\n",
    "    return sparse.coo_matrix((np.ones_like(row), (row, col)), shape=(len(profile2id), len(show2id))).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)\n",
    "val_tr = numerize(vad_plays_tr)\n",
    "val_te = numerize(vad_plays_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_tr = numerize(test_plays_tr)\n",
    "te_te = numerize(test_plays_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12562\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(len(np.arange(n_users)[np.asarray(train_data.sum(1)).ravel() > 0]))\n",
    "print(len(np.arange(n_users)[np.asarray(val_tr.sum(1)).ravel() > 0]))\n",
    "print(len(np.arange(n_users)[np.asarray(te_tr.sum(1)).ravel() > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.arange(n_users)\n",
    "tr_users = r[np.asarray(train_data.sum(1)).ravel() > 0]\n",
    "val_users = r[np.asarray(val_tr.sum(1)).ravel() > 0]\n",
    "te_users = r[np.asarray(te_tr.sum(1)).ravel() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23660119482528905"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_te.nnz / val_tr.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23533681425460676"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_te.nnz / te_tr.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(f\"drm_data/parsed/{dset}-new\", 'wb') as f:\n",
    "#     pickle.dump((tr_users, val_users, te_users, train_data, val_tr, val_te, te_tr, te_te), f)\n",
    "from implicit.evaluation import train_test_split\n",
    "total = (train_data + val_tr + te_tr + val_te + te_te)\n",
    "train, te = train_test_split(total, 0.8)\n",
    "tr, val = train_test_split(train, 0.875)\n",
    "with open(f'drm_data/parsed/{dset}-parsed', 'wb') as f:\n",
    "    pickle.dump((tr, val, te), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from implicit.evaluation import train_test_split\n",
    "# total = (train_data + val_tr + te_tr + val_te + te_te)\n",
    "# train, te = train_test_split(total, 0.8)\n",
    "# tr, val = train_test_split(train, 0.875)\n",
    "# with open(f'data/parsed/{dset}-712-', 'wb') as f:\n",
    "#     pickle.dump((tr, val, te), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<15562x28675 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 391240 stored elements in Compressed Sparse Row format>, <15562x28675 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 55761 stored elements in Compressed Sparse Row format>, <15562x28675 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 111911 stored elements in Compressed Sparse Row format>)\n"
     ]
    }
   ],
   "source": [
    "with open(f'drm_data/parsed/{dset}-parsed', 'rb') as f:\n",
    "    print(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
