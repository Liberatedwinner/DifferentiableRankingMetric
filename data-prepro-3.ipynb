{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ita/code/neurank/code\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘drm_data’: File exists\n",
      "mkdir: cannot create directory ‘drm_data/parsed’: File exists\n"
     ]
    }
   ],
   "source": [
    "# please input the dataset name in this list: ['ml-20m', 'sk', 'epinion', 'melon']\n",
    "!mkdir drm_data\n",
    "!mkdir drm_data/parsed\n",
    "dset = 'ml-20m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          userId  movieId  rating   timestamp\n",
      "0              1        2     3.5  1112486027\n",
      "1              1       29     3.5  1112484676\n",
      "2              1       32     3.5  1112484819\n",
      "3              1       47     3.5  1112484727\n",
      "4              1       50     3.5  1112484580\n",
      "...          ...      ...     ...         ...\n",
      "20000258  138493    68954     4.5  1258126920\n",
      "20000259  138493    69526     4.5  1259865108\n",
      "20000260  138493    69644     3.0  1260209457\n",
      "20000261  138493    70286     5.0  1258126944\n",
      "20000262  138493    71619     2.5  1255811136\n",
      "\n",
      "[20000263 rows x 4 columns]\n",
      "          userId  movieId  rating          ts\n",
      "6              1      151     4.0  1094785734\n",
      "7              1      223     4.0  1112485573\n",
      "8              1      253     4.0  1112484940\n",
      "9              1      260     4.0  1112484826\n",
      "10             1      293     4.0  1112484703\n",
      "...          ...      ...     ...         ...\n",
      "20000256  138493    66762     4.5  1255805408\n",
      "20000257  138493    68319     4.5  1260209720\n",
      "20000258  138493    68954     4.5  1258126920\n",
      "20000259  138493    69526     4.5  1259865108\n",
      "20000261  138493    70286     5.0  1258126944\n",
      "\n",
      "[9995410 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "min_uc=5\n",
    "min_sc=3\n",
    "\n",
    "if dset == 'ml-20m':\n",
    "    !wget -c -O drm_data/ml-20m.zip http://files.grouplens.org/datasets/movielens/ml-20m.zip\n",
    "    !unzip -d drm_data/ drm_data/ml-20m.zip\n",
    "    raw_data = pd.read_csv(os.path.join('drm_data', 'ml-20m', 'ratings.csv'))\n",
    "    print(raw_data)\n",
    "    raw_data.columns = ['userId', 'movieId', 'rating', 'ts']\n",
    "    raw_data = raw_data[raw_data['rating'] > 3.5]\n",
    "    n_heldout_users = 10000\n",
    "    print(raw_data)\n",
    "    \n",
    "    \n",
    "elif dset == 'sk':\n",
    "    !mkdir drm_data/sk\n",
    "    !wget -c -O drm_data/sk/model_likes_anon.psv https://raw.githubusercontent.com/EthanRosenthal/rec-a-sketch/master/data/model_likes_anon.psv\n",
    "    raw_data = pd.read_csv(os.path.join('drm_data', 'sk', 'model_likes_anon.psv'), \n",
    "                           delimiter='|', \n",
    "                           quotechar='\\\\')\n",
    "    print(raw_data)\n",
    "    raw_data['userId'] = raw_data['uid'].astype(\"object\")\n",
    "    raw_data['movieId'] = raw_data['mid'].astype(\"object\")\n",
    "    n_heldout_users = 1500\n",
    "    min_sc = 4\n",
    "    \n",
    "elif dset == 'melon':\n",
    "    raw_data = pd.read_json(os.path.join('drm_data', 'melon', 'train.json'))\n",
    "    print(raw_data)\n",
    "    rows = []\n",
    "    cols = []\n",
    "    for i, r in raw_data.iterrows():\n",
    "        rows.extend([i] * len(r.songs))\n",
    "        cols.extend(r.songs)\n",
    "    raw_data = pd.DataFrame({\"userId\": rows, \"movieId\": cols})\n",
    "    min_sc = 10\n",
    "    n_heldout_users = 10000\n",
    "\n",
    "elif dset == 'epinion':\n",
    "    !wget -c -O drm_data/epinion.zip https://www.cse.msu.edu/~tangjili/datasetcode/epinions_with_rating_timestamp.zip\n",
    "#     !unzip -d drm_data/ drm_data/epinion.zip\n",
    "    import scipy.io\n",
    "    rat = scipy.io.loadmat(\"drm_data/epinion_with_rating_timestamp/rating_with_timestamp.mat\")['rating_with_timestamp']\n",
    "    u = rat[:, 0]\n",
    "    i = rat[:, 1]\n",
    "    r = rat[:, 3]\n",
    "    raw_data = pd.DataFrame({\n",
    "        'userId' : u,\n",
    "        'movieId': i,\n",
    "        'rating': r,\n",
    "    })\n",
    "    print(raw_data)\n",
    "    n_heldout_users = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep items that are clicked on by at least 5 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=min_uc, min_sc=min_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 9983789 watching events from 136675 users and 15529 movies (sparsity: 99.530%)\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1. - _raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (_raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = _raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116675, 10000, 10000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_users), len(vd_users), len(te_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
    "unique_sid = pd.unique(train_plays['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        \n",
    "        n_items_u = len(group)\n",
    "        if n_items_u >= min_uc:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"%d users sampled\" % (1+i))\n",
    "            sys.stdout.flush()\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 users sampled\n",
      "200 users sampled\n",
      "300 users sampled\n",
      "400 users sampled\n",
      "500 users sampled\n",
      "600 users sampled\n",
      "700 users sampled\n",
      "800 users sampled\n",
      "900 users sampled\n",
      "1000 users sampled\n",
      "1100 users sampled\n",
      "1200 users sampled\n",
      "1300 users sampled\n",
      "1400 users sampled\n",
      "1500 users sampled\n",
      "1600 users sampled\n",
      "1700 users sampled\n",
      "1800 users sampled\n",
      "1900 users sampled\n",
      "2000 users sampled\n",
      "2100 users sampled\n",
      "2200 users sampled\n",
      "2300 users sampled\n",
      "2400 users sampled\n",
      "2500 users sampled\n",
      "2600 users sampled\n",
      "2700 users sampled\n",
      "2800 users sampled\n",
      "2900 users sampled\n",
      "3000 users sampled\n",
      "3100 users sampled\n",
      "3200 users sampled\n",
      "3300 users sampled\n",
      "3400 users sampled\n",
      "3500 users sampled\n",
      "3600 users sampled\n",
      "3700 users sampled\n",
      "3800 users sampled\n",
      "3900 users sampled\n",
      "4000 users sampled\n",
      "4100 users sampled\n",
      "4200 users sampled\n",
      "4300 users sampled\n",
      "4400 users sampled\n",
      "4500 users sampled\n",
      "4600 users sampled\n",
      "4700 users sampled\n",
      "4800 users sampled\n",
      "4900 users sampled\n",
      "5000 users sampled\n",
      "5100 users sampled\n",
      "5200 users sampled\n",
      "5300 users sampled\n",
      "5400 users sampled\n",
      "5500 users sampled\n",
      "5600 users sampled\n",
      "5700 users sampled\n",
      "5800 users sampled\n",
      "5900 users sampled\n",
      "6000 users sampled\n",
      "6100 users sampled\n",
      "6200 users sampled\n",
      "6300 users sampled\n",
      "6400 users sampled\n",
      "6500 users sampled\n",
      "6600 users sampled\n",
      "6700 users sampled\n",
      "6800 users sampled\n",
      "6900 users sampled\n",
      "7000 users sampled\n",
      "7100 users sampled\n",
      "7200 users sampled\n",
      "7300 users sampled\n",
      "7400 users sampled\n",
      "7500 users sampled\n",
      "7600 users sampled\n",
      "7700 users sampled\n",
      "7800 users sampled\n",
      "7900 users sampled\n",
      "8000 users sampled\n",
      "8100 users sampled\n",
      "8200 users sampled\n",
      "8300 users sampled\n",
      "8400 users sampled\n",
      "8500 users sampled\n",
      "8600 users sampled\n",
      "8700 users sampled\n",
      "8800 users sampled\n",
      "8900 users sampled\n",
      "9000 users sampled\n",
      "9100 users sampled\n",
      "9200 users sampled\n",
      "9300 users sampled\n",
      "9400 users sampled\n",
      "9500 users sampled\n",
      "9600 users sampled\n",
      "9700 users sampled\n",
      "9800 users sampled\n",
      "9900 users sampled\n",
      "10000 users sampled\n"
     ]
    }
   ],
   "source": [
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 users sampled\n",
      "200 users sampled\n",
      "300 users sampled\n",
      "400 users sampled\n",
      "500 users sampled\n",
      "600 users sampled\n",
      "700 users sampled\n",
      "800 users sampled\n",
      "900 users sampled\n",
      "1000 users sampled\n",
      "1100 users sampled\n",
      "1200 users sampled\n",
      "1300 users sampled\n",
      "1400 users sampled\n",
      "1500 users sampled\n",
      "1600 users sampled\n",
      "1700 users sampled\n",
      "1800 users sampled\n",
      "1900 users sampled\n",
      "2000 users sampled\n",
      "2100 users sampled\n",
      "2200 users sampled\n",
      "2300 users sampled\n",
      "2400 users sampled\n",
      "2500 users sampled\n",
      "2600 users sampled\n",
      "2700 users sampled\n",
      "2800 users sampled\n",
      "2900 users sampled\n",
      "3000 users sampled\n",
      "3100 users sampled\n",
      "3200 users sampled\n",
      "3300 users sampled\n",
      "3400 users sampled\n",
      "3500 users sampled\n",
      "3600 users sampled\n",
      "3700 users sampled\n",
      "3800 users sampled\n",
      "3900 users sampled\n",
      "4000 users sampled\n",
      "4100 users sampled\n",
      "4200 users sampled\n",
      "4300 users sampled\n",
      "4400 users sampled\n",
      "4500 users sampled\n",
      "4600 users sampled\n",
      "4700 users sampled\n",
      "4800 users sampled\n",
      "4900 users sampled\n",
      "5000 users sampled\n",
      "5100 users sampled\n",
      "5200 users sampled\n",
      "5300 users sampled\n",
      "5400 users sampled\n",
      "5500 users sampled\n",
      "5600 users sampled\n",
      "5700 users sampled\n",
      "5800 users sampled\n",
      "5900 users sampled\n",
      "6000 users sampled\n",
      "6100 users sampled\n",
      "6200 users sampled\n",
      "6300 users sampled\n",
      "6400 users sampled\n",
      "6500 users sampled\n",
      "6600 users sampled\n",
      "6700 users sampled\n",
      "6800 users sampled\n",
      "6900 users sampled\n",
      "7000 users sampled\n",
      "7100 users sampled\n",
      "7200 users sampled\n",
      "7300 users sampled\n",
      "7400 users sampled\n",
      "7500 users sampled\n",
      "7600 users sampled\n",
      "7700 users sampled\n",
      "7800 users sampled\n",
      "7900 users sampled\n",
      "8000 users sampled\n",
      "8100 users sampled\n",
      "8200 users sampled\n",
      "8300 users sampled\n",
      "8400 users sampled\n",
      "8500 users sampled\n",
      "8600 users sampled\n",
      "8700 users sampled\n",
      "8800 users sampled\n",
      "8900 users sampled\n",
      "9000 users sampled\n",
      "9100 users sampled\n",
      "9200 users sampled\n",
      "9300 users sampled\n",
      "9400 users sampled\n",
      "9500 users sampled\n",
      "9600 users sampled\n",
      "9700 users sampled\n",
      "9800 users sampled\n",
      "9900 users sampled\n",
      "10000 users sampled\n"
     ]
    }
   ],
   "source": [
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data into (user_index, item_index) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    row, col = tp.userId.apply(lambda x: profile2id[x]).tolist(), tp.movieId.apply(lambda x: show2id[x]).tolist()\n",
    "    return sparse.coo_matrix((np.ones_like(row), (row, col)), shape=(len(profile2id), len(show2id))).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = numerize(train_plays)\n",
    "val_tr = numerize(vad_plays_tr)\n",
    "val_te = numerize(vad_plays_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_tr = numerize(test_plays_tr)\n",
    "te_te = numerize(test_plays_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116675\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(np.arange(n_users)[np.asarray(train_data.sum(1)).ravel() > 0]))\n",
    "print(len(np.arange(n_users)[np.asarray(val_tr.sum(1)).ravel() > 0]))\n",
    "print(len(np.arange(n_users)[np.asarray(te_tr.sum(1)).ravel() > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.arange(n_users)\n",
    "tr_users = r[np.asarray(train_data.sum(1)).ravel() > 0]\n",
    "val_users = r[np.asarray(val_tr.sum(1)).ravel() > 0]\n",
    "te_users = r[np.asarray(te_tr.sum(1)).ravel() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2414057886427456"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_te.nnz / val_tr.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24155100167441546"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_te.nnz / te_tr.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(f\"drm_data/parsed/{dset}-new\", 'wb') as f:\n",
    "#     pickle.dump((tr_users, val_users, te_users, train_data, val_tr, val_te, te_tr, te_te), f)\n",
    "from implicit.evaluation import train_test_split\n",
    "total = (train_data + val_tr + te_tr + val_te + te_te)\n",
    "train, te = train_test_split(total, 0.8)\n",
    "tr, val = train_test_split(train, 0.875)\n",
    "with open(f'drm_data/parsed/{dset}-parsed', 'wb') as f:\n",
    "    pickle.dump((tr, val, te), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from implicit.evaluation import train_test_split\n",
    "# total = (train_data + val_tr + te_tr + val_te + te_te)\n",
    "# train, te = train_test_split(total, 0.8)\n",
    "# tr, val = train_test_split(train, 0.875)\n",
    "# with open(f'data/parsed/{dset}-712-', 'wb') as f:\n",
    "#     pickle.dump((tr, val, te), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<136675x15525 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 6990505 stored elements in Compressed Sparse Row format>, <136675x15525 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 997690 stored elements in Compressed Sparse Row format>, <136675x15525 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 1995581 stored elements in Compressed Sparse Row format>)\n"
     ]
    }
   ],
   "source": [
    "with open(f'drm_data/parsed/{dset}-parsed', 'rb') as f:\n",
    "    print(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
